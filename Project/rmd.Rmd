---
title: "Project 2017"
author: "Lucien Ledune"
date: "1 d√©cembre 2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align='center', fig.width = 10, fig.height = 5)

```

##Introduction 

In this project, we are going to discuss the estimation of the cumulative distribution and the density function of an exponentially distributed random variable. 
We will consider two different cases for the density estimator, 2nd order and 4th order kernels. 

Then we will perform a Monte Carlo simulation on the IMSE of our estimators. Our goal here will be to investigate the rates of convergence of the three estimators. 

##Preparation of data 

In order to simulate a random variable, we will generate X points using rexp(). This will give us points following the exponential distribution. 

We can plot the (ordered) estimated data here and make sure it follows an exponential distribution. 
```{r}
set.seed(856)

X = rexp(500)

plot(sort(X))
```

##Nonparametric estimation of the cumulative density function 

If we want to determine what the function's cumulative density function is, a simple way to do that is to use the given points to calculate the following distribution : 

* 0   if  $x <= X_{(1)}$
* $k/n$   if  $X_{(k)} <= x < X_{(k+1)}$ for $k = 1,...,n-1$
* 1 if   $x  >= X_{(n)}$

We are basically calculating the proportion of data under a point x. 

Let's apply this to our function : 

```{r}
Ycdf = NULL
n = length(X)
Xcdf = seq(0,5,0.1)

for(i in 1:length(Xcdf)){
  Ycdf[i] = sum(X < Xcdf[i])/n
}

plot(Xcdf, Ycdf, main = "Estimation")
plot(Xcdf, pexp(Xcdf), main = "Exponential function")
```

We can easily observe that our 'non-parametric' estimation of the exponential cdf is close to reality. This method is very simple and intuitive, but as we can see here it is very good for estimating the cdf of an unknown function. 

##Nonparametric estimation of density function

We are now going to discuss the density estimation of a function. The simple intuition behind density estimation is that we want to know how many % of the data falls between the interval [k, k+h], h being the wideness of our interval.  

To get smoother result, we can use a kernel estimator, it is going to estimate the value of our point at the x points around it. We are basically giving the data points a "continuous" value, wich results in a smoother estimation. 

<center>$K = 1/{nh} \sum_{i=1}^{n}({{x-X_i}/h})$</center>

Let's apply this to our data. 
First we build the Kernel function and the density estimation function. 
We are going to use Epanechnikov's second order kernel and gaussian kernel and compare them : 
<center>$K_{epach}(u) = 0,75 * (1 - u^2) * (\lvert u \rvert <= 1)$</center>
(Gaussian kernel is simply the gaussian density formula).

We also need to find the optimal bandwith h, there is multiple methods for that but for now we are going to use the rule of thumb. 


```{r}
Kepach = function(u){ 0.75 * (1 - u^2) * (abs(u) <= 1) } #Epanechnikov kernel
Kgauss <- function(u) dnorm(u)  #Gaussian kernel  


rot <- function(X, K) {#Optimal h
    RK <- integrate(function(u) K(u)^2, -Inf, Inf)$value ## integrated square kernel
    mu2 <- integrate(function(u) u^2 * K(u), -Inf, Inf)$value ## variance kernel 
    R <- quantile(X, 0.75) - quantile(X, 0.25) ## interquartile range
    sig <- min(sd(X), R/1.349)
    return(((8 * sqrt(pi) * RK)/(3 * mu2^2))^0.2 * sig * length(X)^(-0.2))
}

hOptEpach = rot(X, Kepach)
hOptGauss = rot(X, Kgauss)

DensEst = function(x, X, h, K) mean(K((x - X)/h))/h #Density calculation 

```
<center>$hopt_{epach} = 0,55$
$hopt_{gauss} = 0,25$</center>

Now we apply the density estimator function to some x points so we can plot it. 
```{r}
densKEpach = sapply(Xcdf, function(Xcdf) DensEst(Xcdf, X, hOptEpach, Kepach))
densKGauss = sapply(Xcdf, function(Xcdf) DensEst(Xcdf, X, hOptGauss, Kgauss))
par(mfrow = c(1,2))
plot(Xcdf, densKEpach, main = "Density estimation (Epanechnikov)", type = 'l')
plot(Xcdf, densKGauss, main = "Density estimation (Gauss)", type = 'l')

```
And we have a function quite similar to the real exponential value (obviously). 

This estimation was done using Epanechnikov's Kernel, which is the most efficient (even if the difference of efficiency between commonly used kernels is very small). 

##Higher order kernel : fourth order 

We will now perform the same density estimation, but using a second order kernel. Those are often called 'bias reduction kernels'. 

Fourth order Epanechnikov kernel : 

<center>$K_{epach4th} = 15/8 * (1 - 7/3 * u^2) * K(u)$</center>

Fourth order Gaussian kernel :

<center>$K_{gauss4th} = 1/2 * (3 - u^2) * K_{gauss}(u)$</center>

One important thing to note with higher order order kernels is that they can be negative for some points. 
Example : 
```{r, echo = FALSE}
u <- seq(-2,2,len=100)
mu2 <- integrate(function(u) u^2*Kepach(u), -Inf, Inf)$value
mu4 <- integrate(function(u) u^4*Kepach(u), -Inf, Inf)$value
Kepan4 <- function(u) (mu4-mu2*u^2)*Kepach(u)/(mu4-mu2^2) #Epanechnikov kernel of order 4
plot(u, sapply(u, function(u) Kepan4(u)), type="l", ylab="", main = "Epanechnikov 4th order kernel")

```


```{r}
# Higher order kernel construction
prDensEstd <- function(x,X,h,K,k) mean(K((x-X)/h))/(h^(k+1))


mu2 <- integrate(function(u) u^2*Kepach(u), -Inf, Inf)$value
mu4 <- integrate(function(u) u^4*Kepach(u), -Inf, Inf)$value
Kepach4 <- function(u) (mu4-mu2*u^2)*Kepach(u)/(mu4-mu2^2) #Epanechnikov kernel of order 4

mug2 <- integrate(function(u) u^2*Kgauss(u), -Inf, Inf)$value
mug4 <- integrate(function(u) u^4*Kgauss(u), -Inf, Inf)$value
Kgauss4 <- function(u) (mug4-mug2*u^2)*Kgauss(u)/(mug4-mug2^2) #gaussian kernel of order 4


hgauss4 = 0.25
hepach4 = 0.55
par(mfrow = c(1,2))
densKEpach4 = sapply(Xcdf, function(Xcdf) prDensEstd(Xcdf, X, hepach4, Kepach4,0))
densKGauss4 = sapply(Xcdf, function(Xcdf) prDensEstd(Xcdf, X, hgauss4, Kgauss4,0))
plot(Xcdf, densKEpach4, main = "Density estimation (Epanechnikov) 4th order", type = 'l')
plot(Xcdf, densKGauss4, main = "Density estimation (Gaussian) 4th order", type = 'l')

```

##Monte carlo study on the MISE 

We are now going to perform a monte carlo study on our different estimators to see how is affected the MISE by sample size changes. 

The mean integrated squared error is calculated as :

<center>$\int E((f_p(x) - f(x))^2)$</center> 


```{r}
#We will do the experience with gaussian kernel. 
 Kn = 1000 #replications


 K = Kgauss #Gaussian kernel 
 fx = function(x){exp(-x)^2}#density from exponential (squared for formula)
 #The second derivative of exp(-x) is exp(-x) !!! (f''(x))
 
MISE = function(K, h, n){
  mu2 <- integrate(function(u) u^2*Kgauss(u), -Inf, Inf)$value
  RK <- integrate(function(u) K(u)^2, -Inf, Inf)$value ## integrated square kernel
  mise = (1/n*h)*RK + h^4/4 * integrate(fx, 0, Inf)$value * mu2^2 ##Formula to estimate MISE
  return(mise)
}


montecarlo = function(Kn, n, Kernel, h){
  MiseK = NULL #Init stock matrix
  for(i in 1:Kn){
    MiseK[i] = MISE(Kernel, h, n)
  }
  return(mean(MiseK))
}

Ns = c(10,25,50,100,250,500,1000) #Different numbers to try 
MiseN = NULL #Init stock matrix 

#Montecarlo for all n 
for(i in 1:length(Ns)){
  MiseN[i] = montecarlo(Kn, Ns[i], Kgauss, hOptGauss)
}


```

